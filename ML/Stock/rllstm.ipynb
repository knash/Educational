{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e6a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from builtins import super\n",
    "\"\"\"\n",
    "Implementation of Block Neural Autoregressive Flow\n",
    "http://arxiv.org/abs/1904.04676\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torchvision\n",
    "print (\"torch\",torch.__version__)\n",
    "print (\"torchvision\",torchvision.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import ROOT\n",
    "import array\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pprint\n",
    "import scipy\n",
    "from scipy import special\n",
    "from functools import partial\n",
    "import json\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from sklearn.manifold import TSNE\n",
    "import csv\n",
    "from torch.utils import data\n",
    "from bnaf import *\n",
    "from tqdm import trange\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--cuda', type=int, default=0, help='Which GPU to run on.')\n",
    "parser.add_argument('--n_steps', type=int, default=100, help='Number of steps to train.')\n",
    "parser.add_argument('--batch_size', type=int, default=200, help='Training batch size.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Data\n",
    "# --------------------\n",
    "matplotlib.use('tkagg')\n",
    "class sample:\n",
    "    def __init__(self, nchan ,biasmag, batch_size, topred, seed=None):\n",
    "        self.batch_size=batch_size\n",
    "        self.nchan=nchan\n",
    "        self.seed=seed\n",
    "        self.topred=topred\n",
    "    def sreturn(self):\n",
    "        xvals=[]\n",
    "        yvals=[]\n",
    "        ychan=[]\n",
    "        if (self.seed==None):\n",
    "                rng = np.random\n",
    "        else:\n",
    "                rng = np.random.RandomState(self.seed)\n",
    "        #print (-1,1,(self.batch_size,self.nchan))\n",
    "        randonarr=rng.uniform(-1,1,(self.batch_size,self.nchan))*self.biasmag\n",
    "        #randonarr[:,1]=1.0*randonarr[:,0]+0.0*randonarr[:,1]\n",
    "\n",
    "\n",
    "        files = os.listdir()\n",
    "        ntimep = 100+self.topred\n",
    "        for bb in range(self.batch_size):\n",
    "\n",
    "\n",
    "                curx=[]\n",
    "                cury=[]\n",
    "                prevarrand=np.array([])\n",
    "                perm=np.arange(4)\n",
    "                np.random.shuffle(perm)\n",
    "                #print(perm)\n",
    "                ychan.append(perm)\n",
    "                for nn in range(self.nchan):\n",
    "                        sinfac=rng.uniform(0.2,1.)*0.7\n",
    "                        sinarr=np.sin(np.arange(ntimep)*sinfac)*5\n",
    "                        cosfac=rng.uniform(0.2,1.)*0.7\n",
    "                        cosarr=np.cos(np.arange(ntimep)*cosfac)*5\n",
    "                        #print(sinfac)\n",
    "                        #print(cosfac)\n",
    "                        corrnoise=True\n",
    "                        arrand = rng.uniform(-1,1,ntimep)\n",
    "                        if corrnoise:\n",
    "                                if prevarrand.size==0:\n",
    "                                        arrand = rng.uniform(-1,1,ntimep)\n",
    "                                else:\n",
    "                                        arrand = rng.uniform(-1,1,ntimep)\n",
    "                                        arrand[nn*5:]=prevarrand[0:-1*nn*5]\n",
    "                                cs=np.cumsum(arrand)\n",
    "                        else:\n",
    "                                arrand = rng.uniform(-1,1,ntimep)\n",
    "                                cs=np.cumsum(arrand+randonarr[bb][nn])+sinarr+cosarr\n",
    "\n",
    "                        #print (sinarr) \n",
    "                        curx.append(cs[:-self.topred])\n",
    "                        cury.append(cs[-self.topred:])\n",
    "                        #curx.append(np.cumsum(arrand+randonarr[bb][0]))\n",
    "                        prevarrand=arrand\n",
    "                curx=np.array(curx)\n",
    "                cury=np.array(cury)\n",
    "                #print (ychan[-1])\n",
    "                #print (curx[:,0])\n",
    "\n",
    "                curx=curx[ychan[-1]]\n",
    "                #curx=np.transpose(curx,ychan[-1])\n",
    "                #print (curx[:,0])\n",
    "                #print ()\n",
    "                cury=cury[ychan[-1]]\n",
    "     \n",
    "                xvals.append((curx))\n",
    "                yvals.append((cury))\n",
    "                #print (np.array(xvals).shape)\n",
    "                #print (np.array(yvals).shape)\n",
    "                #plt.plot(curx)\n",
    "                #plt.xlabel(\"steps\")\n",
    "                #plt.ylabel(\"position\")\n",
    "                #plt.title(\"randdwalk\")\n",
    "                #plt.show()\n",
    "                #sys.exit()\n",
    "                #input(\"Press Enter to continue...\")\n",
    "        return np.array(xvals, dtype=\"float32\"),np.array(yvals, dtype=\"float32\"),np.array(ychan, dtype=\"int64\")\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,nchan=4,ntime=100,n_hidden=64,n_layers=10,npoint=5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.n_hidden=n_hidden\n",
    "        self.n_layers=n_layers\n",
    "        self.ntime=ntime+npoint\n",
    "        self.nchan=nchan\n",
    "        self.npoint=npoint \n",
    "        self.output_size=self.nchan*self.npoint\n",
    "        self.ks=10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.lin = nn.Sequential(  \n",
    "        nn.Linear(464, 2*400)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.n_hidden*self.npoint, self.output_size)\n",
    "\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=nchan,hidden_size= self.n_hidden, num_layers=5,batch_first=True)\n",
    "        self.lstm1 = nn.LSTM(input_size=1,hidden_size= self.n_hidden, num_layers=5,batch_first=True)\n",
    "        #self.soft = nn.Sequential( nn.Softmax(dim=1))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputlstm=inputs[0]\n",
    "        inputlstmshape=inputlstm.shape\n",
    "        batchs =inputlstmshape[0]\n",
    "        inputconv=inputs[1].unsqueeze(1).transpose(1, 2)\n",
    "        h0c = torch.zeros( 5, batchs, self.n_hidden).to('cuda:'+str(args.cuda))  \n",
    "        c0c = torch.zeros( 5, batchs, self.n_hidden).to('cuda:'+str(args.cuda))\n",
    "        LSc,hiddenc=self.lstm1(inputconv, (h0c,c0c)) \n",
    "\n",
    "\n",
    "        inputlstm=inputlstm.transpose(1, 2)\n",
    "        #print(LSc.shape)\n",
    "        #print(inputlstm.shape)\n",
    "        h0 = torch.zeros( 5, batchs, self.n_hidden).to('cuda:'+str(args.cuda))  \n",
    "        c0 = torch.zeros( 5, batchs, self.n_hidden).to('cuda:'+str(args.cuda))\n",
    "        LS,hidden=self.lstm(inputlstm, hiddenc) \n",
    "        \n",
    "        #output = self.out(torch.cat((LS[:,-1,:],LSc[:,-1,:]),dim=1))\n",
    "        LS=LS[:,-self.npoint:,:].reshape(LS.size(0), -1)\n",
    "        output = self.out(LS)\n",
    "        #print (output.shape)\n",
    "        return output.reshape(output.shape[0],self.nchan,self.npoint)\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self,nchan=4,ntime=100,n_hidden=64,n_layers=10,npoint=5):\n",
    "        super(Conv, self).__init__()\n",
    "        self.n_hidden=n_hidden\n",
    "        self.n_layers=n_layers\n",
    "        self.ntime=ntime+npoint\n",
    "        self.nchan=nchan\n",
    "        self.npoint=npoint \n",
    "        self.output_size=self.nchan*self.npoint\n",
    "        self.ks=10\n",
    "\n",
    "        self.conv1 = nn.Sequential( \n",
    "            nn.Conv1d(self.nchan, self.n_hidden,kernel_size=10, padding=1),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential( \n",
    "            nn.Conv1d(self.n_hidden, self.n_hidden,stride=1, kernel_size=30, padding=1),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "        self.lin = nn.Sequential(  \n",
    "        nn.Linear(self.n_hidden*9, self.n_hidden)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.n_hidden, self.nchan*self.nchan)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batchs =inputs.shape[0]\n",
    "        CO = F.relu(self.conv1(inputs))      \n",
    "        CO = F.relu(self.conv2(CO))  \n",
    "        lin = F.relu(self.lin(CO.reshape(CO.size(0), -1)))\n",
    "\n",
    "        output = F.softmax(self.out(lin))\n",
    "\n",
    "        return output.reshape(inputs.shape[0],self.nchan,self.nchan),lin\n",
    "\n",
    "hdfpath=\"/cms/knash/sto/\"\n",
    "hdflist=os.listdir(hdfpath)\n",
    "megaframe=pd.DataFrame()\n",
    "\n",
    "for ihd,hd in enumerate(hdflist):\n",
    "        hdstr=hd.replace(\".json\",\"\")\n",
    "        stockframe=pd.read_json(hdfpath+hd)\n",
    "        stockframe.index=stockframe[\"time\"]\n",
    "        stockframe.index=pd.DatetimeIndex(stockframe.index)\n",
    "\n",
    "        stockframe = stockframe.drop(\"time\",axis=1)\n",
    "        stockframe = stockframe.sort_index(axis=0)\n",
    "\n",
    "        stockframe.columns=[[hdstr,hdstr,hdstr,hdstr,hdstr],['close', 'high', 'low', 'open', 'volume']]\n",
    "\n",
    "        #print (stockframe)\n",
    "        if ihd==0:\n",
    "                megaframe = stockframe.copy()\n",
    "                prestr=hdstr\n",
    "        else:\n",
    "                megaframe = pd.concat( [megaframe,stockframe],axis=1)\n",
    "                print (megaframe.shape)\n",
    "                prestr=hdstr\n",
    "\n",
    "#print (megaframe)\n",
    "\n",
    "megaframe=megaframe.dropna(axis=0)\n",
    "megaframe = megaframe.sort_index(axis=0)\n",
    "#days = (megaframe.index.to_period(\"D\")).copy()\n",
    "#days=days.drop_duplicates()\n",
    "startind=0\n",
    "lenth=30\n",
    "for im,mf in enumerate(megaframe.index):\n",
    "        if (im<startind):\n",
    "                continue\n",
    "        returnframe=megaframe.iloc[im:im+lenth]\n",
    "        print (returnframe)\n",
    "\n",
    "        if (returnframe.shape[0]!=lenth):\n",
    "                continue\n",
    "\n",
    "        highs=(returnframe.iloc[:, returnframe.columns.get_level_values(1)=='high'])\n",
    "        lows=(returnframe.iloc[:, returnframe.columns.get_level_values(1)=='low'])\n",
    "        vol=(returnframe.iloc[:, returnframe.columns.get_level_values(1)=='volume'])\n",
    "        highs.columns=highs.columns.droplevel(level=0)\n",
    "        lows.columns=lows.columns.droplevel(level=0)\n",
    "        vol.columns=vol.columns.droplevel(level=0)\n",
    "        highs=highs.values\n",
    "        lows=lows.values\n",
    "\n",
    "        mean= 0.5*(highs+lows)\n",
    "        nrange=np.max(highs)-np.min(lows)\n",
    "        highs=(highs-mean)/nrange\n",
    "        lows=(lows-mean)/nrange\n",
    "\n",
    "        randonorm=np.random.normal(size=highs.shape)\n",
    "\n",
    "        toreturn=(highs+lows)/2+(100*randonorm*(highs-lows)/2)\n",
    "        print(toreturn)\n",
    "        #if (day.dayofweek)!=0:\n",
    "        #        continue\n",
    "        #curtrainframe=megaframe.loc[str(day):str(day+4)]\n",
    "        #strip17=curtrainframe.index[curtrainframe.index.hour==17]\n",
    "        #curtrainframe=curtrainframe.drop(strip17)\n",
    "        #print(curtrainframe)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "sys.exit()\n",
    "nchantot=4\n",
    "trsamp={}\n",
    "input_size=200\n",
    "ntime=100\n",
    "nepoch=args.n_steps/50\n",
    "biasvsal=0.4\n",
    "topred=30\n",
    "trsamp[\"train\"]=sample(nchantot,biasvsal,input_size,topred)\n",
    "trsamp[\"val\"]=sample(nchantot,biasvsal,input_size,topred,12529)\n",
    "\n",
    "\n",
    "modellstm = LSTM(nchan=nchantot,npoint=topred).to('cuda:'+str(args.cuda))\n",
    "modellstm.train()\n",
    "modelconv = Conv(nchan=nchantot,npoint=topred).to('cuda:'+str(args.cuda))\n",
    "modelconv.train()\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizerlstm = torch.optim.Adam(modellstm.parameters(), lr=0.005, weight_decay=0.0)\n",
    "optimizerconv = torch.optim.Adam(modelconv.parameters(), lr=0.001, weight_decay=0.0)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizerlstm, factor=0.5, patience=0, cooldown=4, verbose=True)\n",
    "fullval=trsamp[\"val\"].sreturn()\n",
    "valsamp=torch.from_numpy(fullval[0]).to('cuda:'+str(args.cuda))\n",
    "valsamptrue=torch.from_numpy(fullval[1]).to('cuda:'+str(args.cuda))\n",
    "fig, axs = plt.subplots(4)\n",
    "with tqdm(total=args.n_steps, desc='Start step {}; Training for {} steps'.format(0, args.n_steps)) as pbar:\n",
    "        for istep in range(args.n_steps):\n",
    "                curtuple = trsamp[\"train\"].sreturn()\n",
    "                curdata = torch.from_numpy(curtuple[0]).to('cuda:'+str(args.cuda))\n",
    "                #print (curdata.shape)\n",
    "\n",
    "                outputsconv  = modelconv(curdata)\n",
    "                predchantrue  = outputsconv[0]\n",
    "                topass  = outputsconv[1]\n",
    "                outputs  = modellstm((curdata,topass))\n",
    "                #print (outputs[0],curtuple[1])\n",
    "                optimizerlstm.zero_grad()\n",
    "                optimizerconv.zero_grad()\n",
    "                curtrue = torch.from_numpy(curtuple[1]).to('cuda:'+str(args.cuda))\n",
    "                #print (curtuple[2])\n",
    "        \n",
    "                curchantrue = torch.from_numpy(curtuple[2])\n",
    "                curchantrue=F.one_hot(curchantrue).to('cuda:'+str(args.cuda)).float()\n",
    "\n",
    "\n",
    "                curpred = outputs\n",
    "                #print (curpred.shape )\n",
    "                #print (curtrue.shape )\n",
    "                #  curpred=curpred.reshape(curtrue.shape[0],curtrue.shape[1],) \n",
    "\n",
    "                #print (torch.std(curtrue-curpred))\n",
    "                #print (curtrue[0],curpred[0])\n",
    "                \n",
    "                loss = criterion(curpred,curtrue).mean()\n",
    "\n",
    "                lossconv = nn.BCELoss()(predchantrue,curchantrue).mean()\n",
    "\n",
    "                pbar.set_postfix(L='{:.4f}'.format(loss*1000),Lchan='{:.4f}'.format(lossconv*1000))\n",
    "                pbar.update()\n",
    "                lossconv.backward(retain_graph=True)\n",
    "                optimizerconv.step()\n",
    "                loss.backward(retain_graph=False)\n",
    "                optimizerlstm.step()\n",
    "\n",
    "                if (istep%nepoch==0):\n",
    "\n",
    "                        for param_group in optimizerlstm.param_groups:\n",
    "                                print(param_group[\"lr\"])\n",
    "                        for param_group in optimizerconv.param_groups:\n",
    "                                print(param_group[\"lr\"])\n",
    "\n",
    "                        outputsconv  = modelconv(valsamp)\n",
    "                        predchantrue  = outputsconv[0]\n",
    "                        topass  = outputsconv[1]\n",
    "                        outputs  = modellstm((valsamp,topass))\n",
    "\n",
    "                        #outputs= outputs.reshape(valsamptrue.shape[0],valsamptrue.shape[1])\n",
    "\n",
    "                        #print (outputs)\n",
    "                        #print (valsamptrue) \n",
    "                        #print (np.array(xvals).shape)\n",
    "                        #print (np.array(yvals).shape)\n",
    "                        plot=True\n",
    "                        if plot:\n",
    "\n",
    "                                #plt.clf()\n",
    "\n",
    "                                axs[0].cla()\n",
    "                                axs[1].cla()\n",
    "                                axs[2].cla()\n",
    "                                axs[3].cla()\n",
    "                                #plt.plot(np.append(valsamp.cpu().numpy()[0,0,:],outputs.cpu().detach().numpy()[0,0,:]))\n",
    "                                #plt.plot(np.append(valsamp.cpu().numpy()[0,1,:],outputs.cpu().detach().numpy()[0,1,:]))\n",
    "                                axs[0].plot(np.append(valsamp.cpu().numpy()[0,0,:],outputs.cpu().detach().numpy()[0,0,:]))\n",
    "                                axs[1].plot(np.append(valsamp.cpu().numpy()[0,1,:],outputs.cpu().detach().numpy()[0,1,:]))\n",
    "                                axs[2].plot(np.append(valsamp.cpu().numpy()[0,2,:],outputs.cpu().detach().numpy()[0,2,:]))\n",
    "                                axs[3].plot(np.append(valsamp.cpu().numpy()[0,3,:],outputs.cpu().detach().numpy()[0,3,:]))\n",
    "                                #plt.xlabel(\"steps\")\n",
    "                                #plt.ylabel(\"position\")\n",
    "                                #plt.title(\"randdwalk\")\n",
    "                                plt.show(block=False)\n",
    "                                plt.pause(1)\n",
    "\n",
    "                        #sys.exit()\n",
    "                        validation_loss=criterion(outputs,valsamptrue).mean()*1000\n",
    "                        scheduler.step(validation_loss)\n",
    "                        print (validation_loss.tolist())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
